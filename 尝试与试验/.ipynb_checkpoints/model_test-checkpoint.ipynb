{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=dilation,\n",
    "                               bias=False, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=dilation,\n",
    "                               bias=False, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        if residual is None:\n",
    "            residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "basicBlock = BasicBlock(3,3)\n",
    "dummy_input = torch.from_numpy(np.random.random((1,3,128,128)))\n",
    "dummy_input = torch.tensor(dummy_input, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(comment='BasicBlock') as w:\n",
    "    w.add_graph(basicBlock, (dummy_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 2\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        expansion = Bottleneck.expansion\n",
    "        bottle_planes = planes // expansion\n",
    "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(bottle_planes)\n",
    "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
    "                               stride=stride, padding=dilation,\n",
    "                               bias=False, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(bottle_planes)\n",
    "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        if residual is None:\n",
    "            residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out += residual\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "class BottleneckX(nn.Module):\n",
    "    expansion = 2\n",
    "    cardinality = 32\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
    "        super(BottleneckX, self).__init__()\n",
    "        cardinality = BottleneckX.cardinality\n",
    "        # dim = int(math.floor(planes * (BottleneckV5.expansion / 64.0)))\n",
    "        # bottle_planes = dim * cardinality\n",
    "        bottle_planes = planes * cardinality // 32\n",
    "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(bottle_planes)\n",
    "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
    "                               stride=stride, padding=dilation, bias=False,\n",
    "                               dilation=dilation, groups=cardinality)\n",
    "        self.bn2 = nn.BatchNorm2d(bottle_planes)\n",
    "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        if residual is None:\n",
    "            residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out += residual\n",
    "        out = self.relu3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = Bottleneck(4,4)\n",
    "bottleneckX = BottleneckX(32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dummy_input1 = np.random.random((1,4,128,128))\n",
    "dummy_input2 = np.random.random((1,32,128,128))\n",
    "dummy_input1 = torch.from_numpy(dummy_input1)\n",
    "dummy_input2 = torch.from_numpy(dummy_input2)\n",
    "dummy_input1 = torch.tensor(dummy_input1, dtype = torch.float32)\n",
    "dummy_input2 = torch.tensor(dummy_input2, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(comment='Bottleneck') as w:\n",
    "    w.add_graph(bottleneck, (dummy_input1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(comment='BottleneckX') as w:\n",
    "    w.add_graph(bottleneckX, (dummy_input2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, residual):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, 1,\n",
    "            stride=1, bias=False, padding=(kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # 输入是多个层输出结果\n",
    "        children = x\n",
    "        x = self.conv(torch.cat(x, 1))\n",
    "        x = self.bn(x)\n",
    "        if self.residual:\n",
    "            x += children[0]\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(nn.Module):\n",
    "    '''\n",
    "    self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n",
    "                    level_root=True, root_residual=residual_root)\n",
    "    '''\n",
    "    def __init__(self, levels, block, in_channels, out_channels, stride=1,\n",
    "                 level_root=False, root_dim=0, root_kernel_size=1,\n",
    "                 dilation=1, root_residual=False):\n",
    "        super(Tree, self).__init__()\n",
    "        if root_dim == 0:\n",
    "            root_dim = 2 * out_channels\n",
    "        if level_root:\n",
    "            root_dim += in_channels\n",
    "        if levels == 1:\n",
    "            self.tree1 = block(in_channels, out_channels, stride,\n",
    "                               dilation=dilation)\n",
    "            self.tree2 = block(out_channels, out_channels, 1,\n",
    "                               dilation=dilation)\n",
    "        else:\n",
    "            self.tree1 = Tree(levels - 1, block, in_channels, out_channels,\n",
    "                              stride, root_dim=0,\n",
    "                              root_kernel_size=root_kernel_size,\n",
    "                              dilation=dilation, root_residual=root_residual)\n",
    "            self.tree2 = Tree(levels - 1, block, out_channels, out_channels,\n",
    "                              root_dim=root_dim + out_channels,\n",
    "                              root_kernel_size=root_kernel_size,\n",
    "                              dilation=dilation, root_residual=root_residual)\n",
    "        if levels == 1:\n",
    "            self.root = Root(root_dim, out_channels, root_kernel_size,\n",
    "                             root_residual)\n",
    "        self.level_root = level_root\n",
    "        self.root_dim = root_dim\n",
    "        self.downsample = None\n",
    "        self.project = None\n",
    "        self.levels = levels\n",
    "        if stride > 1:\n",
    "            self.downsample = nn.MaxPool2d(stride, stride=stride)\n",
    "        if in_channels != out_channels:\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, residual=None, children=None):\n",
    "        children = [] if children is None else children\n",
    "\n",
    "        bottom = self.downsample(x) if self.downsample else x\n",
    "        # project就是映射，如果输入输出通道数不同则将输入通道数映射到输出通道数\n",
    "        residual = self.project(bottom) if self.project else bottom\n",
    "\n",
    "        if self.level_root:\n",
    "            children.append(bottom)\n",
    "\n",
    "        x1 = self.tree1(x, residual)\n",
    "        if self.levels == 1:\n",
    "            x2 = self.tree2(x1)\n",
    "            # root是出口\n",
    "            x = self.root(x2, x1, *children)\n",
    "        else:\n",
    "            children.append(x1)\n",
    "            x = self.tree2(x1, children=children)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_level1 = Tree(1, BasicBlock, 4, 5, 2, level_root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dummy_input3 = np.random.random((1,4,128,128))\n",
    "dummy_input3 = torch.from_numpy(dummy_input3)\n",
    "dummy_input3 = torch.tensor(dummy_input3, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(comment = 'Tree with level 1') as w:\n",
    "    w.add_graph(tree_with_level1, (dummy_input3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_level2 = Tree(2, BasicBlock, 4, 5, 2, level_root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(comment = 'Tree with level 2') as w:\n",
    "    w.add_graph(tree_with_level2, (dummy_input3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLA(nn.Module):\n",
    "    '''\n",
    "    DLA([1, 1, 1, 2, 2, 1],\n",
    "        [16, 32, 64, 128, 256, 512],\n",
    "        block=BasicBlock, **kwargs)\n",
    "    '''\n",
    "    def __init__(self, levels, channels, num_classes=1000,\n",
    "                 block=BasicBlock, residual_root=False, return_levels=False,\n",
    "                 pool_size=7, linear_root=False):\n",
    "        super(DLA, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.return_levels = return_levels\n",
    "        self.num_classes = num_classes\n",
    "        self.base_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n",
    "                      padding=3, bias=False),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(inplace=True))\n",
    "        # 在最初前两层仅仅使用卷积层\n",
    "        self.level0 = self._make_conv_level(\n",
    "            channels[0], channels[0], levels[0])\n",
    "        self.level1 = self._make_conv_level(\n",
    "            channels[0], channels[1], levels[1], stride=2)\n",
    "        '''\n",
    "        if level_root:\n",
    "            root_dim += in_channels\n",
    "        '''\n",
    "        self.level2 = Tree(levels[2], block, channels[1], channels[2], 2,\n",
    "                           level_root=False, root_residual=residual_root)\n",
    "        self.level3 = Tree(levels[3], block, channels[2], channels[3], 2,\n",
    "                           level_root=True, root_residual=residual_root)\n",
    "        self.level4 = Tree(levels[4], block, channels[3], channels[4], 2,\n",
    "                           level_root=True, root_residual=residual_root)\n",
    "        self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n",
    "                           level_root=True, root_residual=residual_root)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(pool_size)\n",
    "        self.fc = nn.Conv2d(channels[-1], num_classes, kernel_size=1,\n",
    "                            stride=1, padding=0, bias=True)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = []\n",
    "        x = self.base_layer(x)\n",
    "        for i in range(6):\n",
    "            # 将几个level串联起来\n",
    "            x = getattr(self, 'level{}'.format(i))(x)\n",
    "            y.append(x)\n",
    "        if self.return_levels:\n",
    "            return y\n",
    "        else:\n",
    "            x = self.avgpool(x)\n",
    "            x = self.fc(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return x\n",
    "        \n",
    "    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n",
    "        modules = []\n",
    "        for i in range(convs):\n",
    "            modules.extend([\n",
    "                nn.Conv2d(inplanes, planes, kernel_size=3,\n",
    "                          stride=stride if i == 0 else 1,\n",
    "                          padding=dilation, bias=False, dilation=dilation),\n",
    "                BatchNorm(planes),\n",
    "                nn.ReLU(inplace=True)])\n",
    "            inplanes = planes\n",
    "        return nn.Sequential(*modules)\n",
    "    \n",
    "    def load_pretrained_model(self,  data='imagenet', name='dla34', hash='ba72cf86'):\n",
    "        fc = self.fc\n",
    "        if name.endswith('.pth'):\n",
    "            model_weights = torch.load(data + name)\n",
    "        else:\n",
    "            model_url = get_model_url(data, name, hash)\n",
    "            model_weights = model_zoo.load_url(model_url)\n",
    "        num_classes = len(model_weights[list(model_weights.keys())[-1]])\n",
    "        self.fc = nn.Conv2d(\n",
    "            self.channels[-1], num_classes,\n",
    "            kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.load_state_dict(model_weights)\n",
    "        self.fc = fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDAUp(nn.Module):\n",
    "    def __init__(self, node_kernel, out_dim, channels, up_factors):\n",
    "        super(IDAUp, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.out_dim = out_dim\n",
    "        for i, c in enumerate(channels):\n",
    "            if c == out_dim:\n",
    "                proj = Identity()\n",
    "            else:\n",
    "                proj = nn.Sequential(\n",
    "                    nn.Conv2d(c, out_dim,\n",
    "                              kernel_size=1, stride=1, bias=False),\n",
    "                    nn.BatchNorm2d(out_dim),\n",
    "                    nn.ReLU(inplace=True))\n",
    "            f = int(up_factors[i])\n",
    "            if f == 1:\n",
    "                up = Identity()\n",
    "            else:\n",
    "                up = nn.ConvTranspose2d(\n",
    "                    out_dim, out_dim, f * 2, stride=f, padding=f // 2,\n",
    "                    output_padding=0, groups=out_dim, bias=False)\n",
    "                fill_up_weights(up)\n",
    "            setattr(self, 'proj_' + str(i), proj)\n",
    "            setattr(self, 'up_' + str(i), up)\n",
    "\n",
    "        for i in range(1, len(channels)):\n",
    "            node = nn.Sequential(\n",
    "                nn.Conv2d(out_dim * 2, out_dim,\n",
    "                          kernel_size=node_kernel, stride=1,\n",
    "                          padding=node_kernel // 2, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU(inplace=True))\n",
    "            setattr(self, 'node_' + str(i), node)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, layers):\n",
    "        assert len(self.channels) == len(layers), \\\n",
    "            '{} vs {} layers'.format(len(self.channels), len(layers))\n",
    "        layers = list(layers)\n",
    "        for i, l in enumerate(layers):\n",
    "            upsample = getattr(self, 'up_' + str(i))\n",
    "            project = getattr(self, 'proj_' + str(i))\n",
    "            layers[i] = upsample(project(l))\n",
    "        x = layers[0]\n",
    "        y = []\n",
    "        for i in range(1, len(layers)):\n",
    "            node = getattr(self, 'node_' + str(i))\n",
    "            x = node(torch.cat([x, layers[i]], 1))\n",
    "            y.append(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLAUp(nn.Module):\n",
    "    def __init__(self, channels, scales=(1, 2, 4, 8, 16), in_channels=None):\n",
    "        super(DLAUp, self).__init__()\n",
    "        if in_channels is None:\n",
    "            in_channels = channels\n",
    "        self.channels = channels\n",
    "        channels = list(channels)\n",
    "        scales = np.array(scales, dtype=int)\n",
    "        for i in range(len(channels) - 1):\n",
    "            j = -i - 2\n",
    "            setattr(self, 'ida_{}'.format(i),\n",
    "                    IDAUp(3, channels[j], in_channels[j:],\n",
    "                          scales[j:] // scales[j]))\n",
    "            scales[j + 1:] = scales[j]\n",
    "            in_channels[j + 1:] = [channels[j] for _ in channels[j + 1:]]\n",
    "\n",
    "    def forward(self, layers):\n",
    "        layers = list(layers)\n",
    "        assert len(layers) > 1\n",
    "        for i in range(len(layers) - 1):\n",
    "            ida = getattr(self, 'ida_{}'.format(i))\n",
    "            x, y = ida(layers[-i - 2:])\n",
    "            layers[-i - 1:] = y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLASeg(nn.Module):\n",
    "    '''\n",
    "    DLASeg('dla{}'.format(num_layers), heads,\n",
    "                 pretrained=True,\n",
    "                 down_ratio=down_ratio,\n",
    "                 final_kernel=1,\n",
    "                 last_level=5,\n",
    "                 head_conv=head_conv)\n",
    "    '''\n",
    "    def __init__(self, base_name, heads, pretrained, down_ratio, final_kernel,\n",
    "                 last_level, head_conv, out_channel=0):\n",
    "        super(DLASeg, self).__init__()\n",
    "        assert down_ratio in [2, 4, 8, 16]\n",
    "        self.first_level = int(np.log2(down_ratio))\n",
    "        self.last_level = last_level\n",
    "        # globals() 函数会以字典类型返回当前位置的全部全局变量。\n",
    "        # 所以这个base就相当于原来的DLA34\n",
    "        self.base = globals()[base_name](pretrained=pretrained)\n",
    "        channels = self.base.channels\n",
    "        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n",
    "        # first_level = 2 if down_ratio=4\n",
    "        # channels = [16, 32, 64, 128, 256, 512] to [64, 128, 256, 512]\n",
    "        # scales = [1, 2, 4, 8]\n",
    "        self.dla_up = DLAUp(self.first_level, channels[self.first_level:], scales)\n",
    "\n",
    "        if out_channel == 0:\n",
    "            out_channel = channels[self.first_level]\n",
    "\n",
    "        # 进行上采样\n",
    "        self.ida_up = IDAUp(out_channel, channels[self.first_level:self.last_level], \n",
    "                            [2 ** i for i in range(self.last_level - self.first_level)])\n",
    "        \n",
    "        self.heads = heads\n",
    "        for head in self.heads:\n",
    "            classes = self.heads[head]\n",
    "            if head_conv > 0:\n",
    "              fc = nn.Sequential(\n",
    "                  nn.Conv2d(channels[self.first_level], head_conv,\n",
    "                    kernel_size=3, padding=1, bias=True),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Conv2d(head_conv, classes, \n",
    "                    kernel_size=final_kernel, stride=1, \n",
    "                    padding=final_kernel // 2, bias=True))\n",
    "              if 'hm' in head:\n",
    "                fc[-1].bias.data.fill_(-2.19)\n",
    "              else:\n",
    "                fill_fc_weights(fc)\n",
    "            else:\n",
    "              fc = nn.Conv2d(channels[self.first_level], classes, \n",
    "                  kernel_size=final_kernel, stride=1, \n",
    "                  padding=final_kernel // 2, bias=True)\n",
    "              if 'hm' in head:\n",
    "                fc.bias.data.fill_(-2.19)\n",
    "              else:\n",
    "                fill_fc_weights(fc)\n",
    "            self.__setattr__(head, fc)\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.dla_up(x)\n",
    "        y = []\n",
    "        for i in range(self.last_level - self.first_level):\n",
    "            y.append(x[i].clone())\n",
    "        self.ida_up(y, 0, len(y))\n",
    "        z = {}\n",
    "        for head in self.heads:\n",
    "            z[head] = self.__getattr__(head)(y[-1])\n",
    "        return [z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
